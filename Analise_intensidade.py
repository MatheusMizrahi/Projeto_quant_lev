"""
Analise_intensidade.py

Aplica K-Means para classificar intensidade dos regimes macroecon√¥micos.
Identifica se um regime √© Fraco, Moderado ou Forte baseado em an√°lise hist√≥rica.
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import warnings
from pathlib import Path

# Ignorar avisos
warnings.simplefilter(action='ignore', category=FutureWarning)


class AnalisadorIntensidade:
    """
    Classe para an√°lise de intensidade de regimes via K-Means.
    
    Attributes:
        n_clusters (int): N√∫mero de clusters (padr√£o: 3 = Fraco/Moderado/Forte)
        kmeans: Modelo K-Means treinado
        scaler: StandardScaler para normaliza√ß√£o
        mapeamento_intensidades: Dict mapeando clusters para intensidades
    """
    
    def __init__(self, n_clusters=3):
        """
        Inicializa o analisador.
        
        Args:
            n_clusters (int): N√∫mero de n√≠veis de intensidade (padr√£o: 3)
        """
        self.n_clusters = n_clusters
        self.kmeans = None
        self.scaler = StandardScaler()
        self.mapeamento_intensidades = None
        self.features_names = None
    
    def preparar_features(self, df):
        """
        Cria features para clusteriza√ß√£o a partir do hist√≥rico.
        
        Features criadas:
        - magnitude: dist√¢ncia euclidiana da origem (for√ßa total do sinal)
        - inflacao_abs: valor absoluto do score de infla√ß√£o
        - atividade_abs: valor absoluto do score de atividade
        - inflacao_vol: volatilidade do score de infla√ß√£o (janela 20 dias)
        - atividade_vol: volatilidade do score de atividade (janela 20 dias)
        - consistencia: % de dias no mesmo quadrante (janela 20 dias)
        
        Args:
            df (DataFrame): Hist√≥rico com colunas [inflacao_score, atividade_score, quadrante]
        
        Returns:
            DataFrame com 6 features normalizadas
        """
        features = pd.DataFrame()
        
        # Feature 1: Magnitude (dist√¢ncia da origem)
        # Quanto mais longe de (0,0), mais forte o sinal
        features['magnitude'] = np.sqrt(
            df['inflacao_score']**2 + 
            df['atividade_score']**2
        )
        
        # Features 2 e 3: Valores absolutos
        # Intensidade independente da dire√ß√£o
        features['inflacao_abs'] = df['inflacao_score'].abs()
        features['atividade_abs'] = df['atividade_score'].abs()
        
        # Features 4 e 5: Volatilidade (janela de 20 dias)
        # Sinal vol√°til = menos confi√°vel
        features['inflacao_vol'] = (
            df['inflacao_score']
            .rolling(window=20, min_periods=5)
            .std()
            .fillna(0)
        )
        features['atividade_vol'] = (
            df['atividade_score']
            .rolling(window=20, min_periods=5)
            .std()
            .fillna(0)
        )
        
        # Feature 6: Consist√™ncia do quadrante
        # Se mudou de quadrante recentemente = sinal fraco
        def calcular_consistencia(serie):
            """Calcula % de dias no mesmo quadrante (√∫ltimos 20)"""
            if len(serie) < 5:
                return 0.5  # Valor neutro
            return (serie == serie.iloc[-1]).sum() / len(serie)
        
        features['consistencia'] = (
            df['quadrante']
            .rolling(window=20, min_periods=5)
            .apply(calcular_consistencia)
            .fillna(0.5)
        )
        
        self.features_names = features.columns.tolist()
        
        return features
    
    def encontrar_k_otimo(self, features_scaled, max_k=6, verbose=True):
        """
        Usa Elbow Method e Silhouette Score para encontrar K √≥timo.
        
        Args:
            features_scaled: Features normalizadas
            max_k (int): M√°ximo de clusters para testar
            verbose (bool): Se True, imprime resultados
        
        Returns:
            DataFrame com m√©tricas para cada K testado
        """
        resultados = {'K': [], 'Inertia': [], 'Silhouette': []}
        
        for k in range(2, max_k + 1):
            if verbose:
                print(f"Testando K={k}...", end=' ')
            
            kmeans = KMeans(
                n_clusters=k,
                random_state=42,
                n_init=10,
                max_iter=300
            )
            labels = kmeans.fit_predict(features_scaled)
            
            inertia = kmeans.inertia_
            silhouette = silhouette_score(features_scaled, labels)
            
            resultados['K'].append(k)
            resultados['Inertia'].append(inertia)
            resultados['Silhouette'].append(silhouette)
            
            if verbose:
                print(f"Inertia={inertia:.2f}, Silhouette={silhouette:.3f}")
        
        df_resultados = pd.DataFrame(resultados)
        
        if verbose:
            # Plotar gr√°ficos
            self._plotar_k_otimo(df_resultados)
        
        return df_resultados
    
    def _plotar_k_otimo(self, df_resultados):
        """
        Cria gr√°ficos de Elbow Method e Silhouette Score.
        
        Args:
            df_resultados: DataFrame com m√©tricas por K
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # Elbow Method
        ax1.plot(df_resultados['K'], df_resultados['Inertia'], 
                'bo-', linewidth=2, markersize=8)
        ax1.set_xlabel('N√∫mero de Clusters (K)', fontsize=12)
        ax1.set_ylabel('In√©rcia (WCSS)', fontsize=12)
        ax1.set_title('Elbow Method', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # Silhouette Score
        ax2.plot(df_resultados['K'], df_resultados['Silhouette'], 
                'ro-', linewidth=2, markersize=8)
        ax2.set_xlabel('N√∫mero de Clusters (K)', fontsize=12)
        ax2.set_ylabel('Silhouette Score', fontsize=12)
        ax2.set_title('Silhouette Analysis', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)
        
        # Marcar melhor K
        melhor_k = df_resultados.loc[df_resultados['Silhouette'].idxmax(), 'K']
        melhor_silhouette = df_resultados['Silhouette'].max()
        ax2.scatter([melhor_k], [melhor_silhouette], 
                   color='green', s=200, zorder=5, marker='*',
                   label=f'Melhor K={melhor_k}')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig('k_otimo_analise.png', dpi=300)
        print("\n‚úì Gr√°ficos salvos em 'k_otimo_analise.png'")
        plt.show()
    
    def treinar(self, df_historico, verbose=True):
        """
        Pipeline completo de treinamento do K-Means.
        
        Passos:
        1. Preparar features
        2. Normalizar
        3. Treinar K-Means
        4. Mapear clusters para intensidades
        
        Args:
            df_historico: DataFrame com hist√≥rico de quadrantes
            verbose (bool): Se True, imprime progresso
        
        Returns:
            tuple: (labels, features)
        """
        if verbose:
            print("\nüî¨ Iniciando treinamento K-Means...\n")
        
        # Passo 1: Preparar features
        if verbose:
            print("1Ô∏è‚É£ Preparando features...")
        features = self.preparar_features(df_historico)
        if verbose:
            print(f"   ‚úì {len(features.columns)} features criadas: {list(features.columns)}")
        
        # Passo 2: Normalizar
        if verbose:
            print("\n2Ô∏è‚É£ Normalizando features...")
        features_scaled = self.scaler.fit_transform(features)
        if verbose:
            print(f"   ‚úì Features padronizadas (m√©dia‚âà0, std‚âà1)")
        
        # Passo 3: Treinar K-Means
        if verbose:
            print(f"\n3Ô∏è‚É£ Treinando K-Means (K={self.n_clusters})...")
        
        self.kmeans = KMeans(
            n_clusters=self.n_clusters,
            random_state=42,
            n_init=10,
            max_iter=300
        )
        labels = self.kmeans.fit_predict(features_scaled)
        
        # Calcular m√©tricas
        silhouette = silhouette_score(features_scaled, labels)
        
        if verbose:
            print(f"   ‚úì Modelo treinado")
            print(f"   ‚úì Silhouette Score: {silhouette:.3f}")
            print(f"   ‚úì Centr√≥ides salvos: {self.kmeans.cluster_centers_.shape}")
            
            # Distribui√ß√£o dos clusters
            print(f"\nüìä Distribui√ß√£o dos clusters:")
            unique, counts = np.unique(labels, return_counts=True)
            for cluster, count in zip(unique, counts):
                pct = (count / len(labels)) * 100
                print(f"   Cluster {cluster}: {count:3} observa√ß√µes ({pct:5.1f}%)")
        
        # Passo 4: Mapear intensidades
        if verbose:
            print("\n4Ô∏è‚É£ Mapeando clusters para intensidades...")
        self.mapeamento_intensidades = self._mapear_intensidades(features, labels, verbose)
        
        if verbose:
            print("\n‚úÖ Treinamento completo!\n")
        
        return labels, features
    
    def _mapear_intensidades(self, features, labels, verbose=True):
        """
        Mapeia clusters num√©ricos para r√≥tulos sem√¢nticos (Fraco/Moderado/Forte).
        
        L√≥gica: Cluster com maior magnitude m√©dia = Forte
        
        Args:
            features: DataFrame de features
            labels: Array de r√≥tulos de cluster
            verbose (bool): Se True, imprime mapeamento
        
        Returns:
            dict: Mapeamento {cluster: intensidade}
        """
        df_temp = features.copy()
        df_temp['cluster'] = labels
        
        # Calcular magnitude m√©dia por cluster
        magnitude_media = df_temp.groupby('cluster')['magnitude'].mean()
        
        if verbose:
            print(f"\n   Magnitude m√©dia por cluster:")
        
        # Ordenar clusters por magnitude (menor ‚Üí maior)
        clusters_ordenados = magnitude_media.sort_values().index.tolist()
        
        # Criar mapeamento
        intensidades = ['Fraco', 'Moderado', 'Forte'] if self.n_clusters == 3 else [f'Nivel_{i}' for i in range(self.n_clusters)]
        mapeamento = {}
        
        for i, cluster in enumerate(clusters_ordenados):
            mapeamento[cluster] = intensidades[i]
            mag = magnitude_media[cluster]
            if verbose:
                print(f"   Cluster {cluster} (mag={mag:.3f}) ‚Üí {intensidades[i]}")
        
        return mapeamento
    
    def classificar(self, inflacao_score, atividade_score, features_contexto=None):
        """
        Classifica intensidade de uma nova observa√ß√£o.
        
        Args:
            inflacao_score (float): Score de infla√ß√£o atual
            atividade_score (float): Score de atividade atual
            features_contexto (dict, optional): Dict com volatilidades e consist√™ncia
                Formato: {'inflacao_vol': float, 'atividade_vol': float, 'consistencia': float}
        
        Returns:
            dict: Classifica√ß√£o completa com intensidade, cluster e magnitude
        """
        if self.kmeans is None:
            raise ValueError("Modelo n√£o treinado! Execute treinar() primeiro.")
        
        # Preparar features do ponto atual
        features_atual = {
            'magnitude': np.sqrt(inflacao_score**2 + atividade_score**2),
            'inflacao_abs': abs(inflacao_score),
            'atividade_abs': abs(atividade_score),
        }
        
        # Adicionar contexto se dispon√≠vel, sen√£o usar valores default
        if features_contexto:
            features_atual['inflacao_vol'] = features_contexto.get('inflacao_vol', 0.05)
            features_atual['atividade_vol'] = features_contexto.get('atividade_vol', 0.05)
            features_atual['consistencia'] = features_contexto.get('consistencia', 0.7)
        else:
            # Valores default (m√©dios)
            features_atual['inflacao_vol'] = 0.05
            features_atual['atividade_vol'] = 0.05
            features_atual['consistencia'] = 0.7
        
        # Converter para array (mesma ordem do treinamento!)
        X = np.array([[
            features_atual['magnitude'],
            features_atual['inflacao_abs'],
            features_atual['atividade_abs'],
            features_atual['inflacao_vol'],
            features_atual['atividade_vol'],
            features_atual['consistencia']
        ]])
        
        # Normalizar usando o scaler treinado
        X_scaled = self.scaler.transform(X)
        
        # Prever cluster
        cluster = self.kmeans.predict(X_scaled)[0]
        
        # Mapear para intensidade
        intensidade = self.mapeamento_intensidades[cluster]
        
        # Calcular dist√¢ncia ao centr√≥ide (medida de confian√ßa)
        distancia = np.linalg.norm(X_scaled - self.kmeans.cluster_centers_[cluster])
        
        return {
            'intensidade': intensidade,
            'cluster': int(cluster),
            'magnitude': features_atual['magnitude'],
            'distancia_centroide': float(distancia),
            'features': features_atual
        }
    
    def adicionar_intensidades_ao_historico(self, df_historico, labels):
        """
        Adiciona colunas de cluster e intensidade ao hist√≥rico.
        
        Args:
            df_historico: DataFrame original
            labels: Array de r√≥tulos de cluster
        
        Returns:
            DataFrame com colunas adicionais [cluster, intensidade]
        """
        df_resultado = df_historico.copy()
        df_resultado['cluster'] = labels
        df_resultado['intensidade'] = [self.mapeamento_intensidades[c] for c in labels]
        
        return df_resultado
    
    def visualizar_clusters(self, df_resultado, salvar=True):
        """
        Cria visualiza√ß√£o 2D dos clusters no espa√ßo Infla√ß√£o √ó Atividade.
        
        Args:
            df_resultado: DataFrame com colunas [inflacao_score, atividade_score, intensidade]
            salvar (bool): Se True, salva gr√°fico em arquivo
        """
        plt.figure(figsize=(12, 8))
        
        # Cores por intensidade
        cores = {'Fraco': 'lightblue', 'Moderado': 'orange', 'Forte': 'red'}
        
        # Scatter plot por intensidade
        intensidades = df_resultado['intensidade'].unique()
        for intensidade in intensidades:
            mask = df_resultado['intensidade'] == intensidade
            plt.scatter(
                df_resultado.loc[mask, 'inflacao_score'],
                df_resultado.loc[mask, 'atividade_score'],
                c=cores.get(intensidade, 'gray'),
                label=intensidade,
                alpha=0.6,
                s=100
            )
        
        # Linhas de separa√ß√£o dos quadrantes
        plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5, alpha=0.3)
        plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5, alpha=0.3)
        
        # Labels dos quadrantes
        max_x = df_resultado['inflacao_score'].abs().max() * 0.7
        max_y = df_resultado['atividade_score'].abs().max() * 0.7
        plt.text(max_x, max_y, 'Q2:\nREFLA√á√ÉO', fontsize=9, alpha=0.3, ha='center', va='center')
        plt.text(-max_x, max_y, 'Q1:\nGOLDILOCKS', fontsize=9, alpha=0.3, ha='center', va='center')
        plt.text(-max_x, -max_y, 'Q4:\nDEFLA√á√ÉO', fontsize=9, alpha=0.3, ha='center', va='center')
        plt.text(max_x, -max_y, 'Q3:\nESTAGFLA√á√ÉO', fontsize=9, alpha=0.3, ha='center', va='center')
        
        plt.xlabel('Score de Infla√ß√£o', fontsize=12)
        plt.ylabel('Score de Atividade Econ√¥mica', fontsize=12)
        plt.title('Classifica√ß√£o de Intensidade dos Regimes (K-Means)', fontsize=14, fontweight='bold')
        plt.legend(title='Intensidade', fontsize=10, loc='upper right')
        plt.grid(True, alpha=0.2)
        
        plt.tight_layout()
        
        if salvar:
            plt.savefig('clusters_intensidade.png', dpi=300)
            print("üìä Gr√°fico salvo em 'clusters_intensidade.png'")
        
        plt.show()


def main():
    """
    Fun√ß√£o principal para executar an√°lise de intensidade.
    """
    print("\n" + "="*70)
    print(" "*15 + "AN√ÅLISE DE INTENSIDADE VIA K-MEANS")
    print("="*70)
    
    # 1. Carregar hist√≥rico
    print("\n1Ô∏è‚É£ Carregando hist√≥rico de quadrantes...")
    try:
        df_historico = pd.read_csv('historico_quadrantes.csv', parse_dates=['data'])
        print(f"   ‚úì {len(df_historico)} observa√ß√µes carregadas")
        print(f"   ‚úì Per√≠odo: {df_historico['data'].min().date()} a {df_historico['data'].max().date()}")
    except FileNotFoundError:
        print("\n   ‚ùå Arquivo 'historico_quadrantes.csv' n√£o encontrado!")
        print("   üí° Execute primeiro 'visualizacao_analise_historica/analise_historica.py'")
        return
    
    # 2. Criar analisador
    print("\n2Ô∏è‚É£ Criando analisador de intensidade...")
    analisador = AnalisadorIntensidade(n_clusters=3)
    print("   ‚úì Analisador criado (K=3: Fraco/Moderado/Forte)")
    
    # 3. Treinar modelo
    labels, features = analisador.treinar(df_historico, verbose=True)
    
    # 4. Adicionar intensidades ao hist√≥rico
    print("\n5Ô∏è‚É£ Adicionando intensidades ao hist√≥rico...")
    df_final = analisador.adicionar_intensidades_ao_historico(df_historico, labels)
    
    # 5. Estat√≠sticas
    print("\nüìä DISTRIBUI√á√ÉO DE INTENSIDADES:")
    print("-" * 70)
    dist = df_final['intensidade'].value_counts()
    for intensidade, count in dist.items():
        pct = (count / len(df_final)) * 100
        print(f"   {intensidade:10} {count:3} per√≠odos ({pct:5.1f}%)")
    
    # 6. Estat√≠sticas por quadrante
    print("\nüìà INTENSIDADE POR QUADRANTE:")
    print("-" * 70)
    crosstab = pd.crosstab(df_final['quadrante'], df_final['intensidade'], normalize='index') * 100
    print(crosstab.round(1))
    
    # 7. Salvar resultados
    print("\n6Ô∏è‚É£ Salvando resultados...")
    df_final.to_csv('historico_com_intensidade.csv', index=False)
    print("   ‚úì Resultados salvos em 'historico_com_intensidade.csv'")
    
    # 8. Visualizar
    print("\n7Ô∏è‚É£ Gerando visualiza√ß√£o...")
    analisador.visualizar_clusters(df_final, salvar=True)
    
    # 9. Classificar observa√ß√£o atual
    print("\n8Ô∏è‚É£ Classificando regime atual...")
    ultima = df_final.iloc[-1]
    
    # Calcular features de contexto (volatilidade e consist√™ncia recentes)
    ultimas_20 = df_final.tail(20)
    features_contexto = {
        'inflacao_vol': ultimas_20['inflacao_score'].std(),
        'atividade_vol': ultimas_20['atividade_score'].std(),
        'consistencia': (ultimas_20['quadrante'] == ultima['quadrante']).sum() / 20
    }
    
    resultado = analisador.classificar(
        ultima['inflacao_score'],
        ultima['atividade_score'],
        features_contexto
    )
    
    # 10. Exibir resultado final
    print("\n" + "="*70)
    print(" "*20 + "üéØ REGIME ATUAL")
    print("="*70)
    print(f"\nüìÖ Data: {ultima['data'].date()}")
    print(f"üìä Quadrante: {ultima['quadrante']}")
    print(f"üí™ Intensidade: {resultado['intensidade']}")
    print(f"üìà Magnitude do Sinal: {resultado['magnitude']:.3f}")
    print(f"üéØ Cluster: {resultado['cluster']}")
    print(f"üìè Dist√¢ncia ao Centr√≥ide: {resultado['distancia_centroide']:.3f}")
    print(f"üîç Consist√™ncia (20 dias): {features_contexto['consistencia']:.1%}")
    print("\n" + "="*70)
    print("\n‚úÖ An√°lise completa!")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
